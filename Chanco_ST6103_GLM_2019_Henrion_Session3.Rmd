---
title: "ST6103 - GLM - Session 3"
author: "Marc Henrion"
date: "17 July 2019"
header-includes:
   - \usepackage{amsmath}
output:
  powerpoint_presentation:
    reference_doc: MlwChanco_Template.pptx
---

```{r setup, include=FALSE, echo=F}
knitr::opts_chunk$set(echo = TRUE, fig.width=16, fig.height=9, dpi=150, highlight=T)

require(tidyverse)
require(knitr)
require(gridExtra)
```


#

## Session 3: Generalised Linear Model - continued


# Notation

* $X, Y$ - random variables (here: X = predictor, Y = response)

* $x, y$ - measured / observed values

* $\epsilon$ - random variable (here: error / residual)

* $\mathbf{\theta}$ - a vector of parameters

* $\bar{X}$, $\bar{Y}$ - sample mean estimators for X, Y

* $\bar{x}$, $\bar{y}$ - sample mean estimates of X, Y

* $\hat{T}$, $\hat{t}$ - given a statistic T, estimator and estimate of T

* $P(A)$ - probability of an event A occuring

* $f_X(.)$, $f_Y(.)$ - distribution mass / density functions of X, Y

* $X\sim F$ - X distributed according to distribution function F

* $E[X]$, $E[Y]$, $E[T]$ - the expectation of X, Y, T respectively

# Generalised linear model

The estimators $\hat{\mathbf{\beta}}$ have the usual statistical porperties of maximum likelihood estimators. Specifically, the $\hat{\mathbf{\beta}}$ are asymptotically normally distributed:

$$\hat{\mathbf{\beta}}\sim\mathcal{N}(\mathbf{\beta},a(\phi)(\mathbf{X}^T\hat{\mathbf{W}}\mathbf{X})^{-1})$$
Estimated standard errors for the $\hat{\mathbf{\beta}}$ are therefore the square roots of the diagonal of

$$Cov(\hat{\mathbf{\beta}})=a(\phi)(\mathbf{X}^T\hat{\mathbf{W}}\mathbf{X})^{-1}$$
Note that $(\mathbf{X}^T\hat{\mathbf{W}}\mathbf{X})^{-1}$ is computed at each IWLS iteration and we can use the one from the final iteration.

When $a(\phi)$ is unknown it must be estimated.


# Generalised linear model

Equipped with standard errors and a distribution for $\hat{\mathbf{\beta}}$ we can compute confidence intervals and perform statistical tests about the parameters.

$(1-\alpha)100\%$ confidence intervals can be found by
$$\,$$
$$\hat{\beta}_j\pm z_{\alpha/2}SE(\hat{\beta}_j)$$
$$\,$$
where $z_{\alpha/2}$ is the $1-{\alpha/2}^{th}$ percentile of the normal distribution (for $\alpha=0.05$, $z_{\alpha/2}\approx1.96$).

# Generalised linear model

We can now do **statistical inference**: we can use statistical theory and the estimated regression coefficients to make statements about the data and hence the processs that gave rise to them.

$$\,$$

For instance we may want to check whether any of the predictor variables does indeed predict the response variable. That is, we want to test whether all or a subset of coefficients are zero.
$$\,$$

There are 3 types of test we can perform:

1. Likelihood ratio tests

2. Wald tests

3. Score tests

# Generalised linear model

We can arrange our vector of parameters $\mathbf{\beta}$ so that it is the bottom $q$ of the $p+1$ coefficients that we want to test.
$$\,$$
$$\mathbf{\beta}=\binom{\mathbf{\beta}_1}{\mathbf{\beta}_2}$$
where $\mathbf{\beta}$ is $(p+1)\times1$, $\mathbf{\beta}_1$ is $(p-q+1)\times1$ and $\mathbf{\beta}_2$ is $q\times1$.
$$\,$$

We can then test $H_0: \mathbf{\beta}_2=\mathbf{\beta}_2^0$ against $H_1:\mathbf{\beta}_2\neq\mathbf{\beta}_2^0$ where $\mathbf{\beta}_2^0$ are some set of fixed values (usually $\mathbf{\beta}_2^0=\mathbf{0}$).

# Generalised linear model

The likelihood ratio test is given by
$$\,$$
$$2[L(\hat{\mathbf{\beta}})-L(\hat{\mathbf{\beta}}_1,\mathbf{\beta}_2^0)]\sim\chi^2_{q}$$

# Generalised linear model

The Wald test is given by
$$(\hat{\mathbf{\beta}}_2-\mathbf{\beta}_2^0)^T(Cov(\hat{\mathbf{\beta}}_2))^{-1}(\hat{\mathbf{\beta}}_2-\mathbf{\beta}_2^0)\sim\chi^2_{q}$$

Wald tests are typically what is reported by statistical software, and for the case where $q=1$, this simplifies to a z-test (i.e. based on the normal distribution; recall: the square of a normal is a $\chi^2_1$).

For $q=1$, we test $H_0:\beta_j=b$ vs. $H_1:\beta_j\neq b$ and the test statistic (using the normal distribution of the MLE estimates) is given by:

$$Z_j=\frac{\hat{\beta}_j-b}{\sqrt{a(\phi)}(\mathbf{X}^T\hat{\mathbf{W}}\mathbf{X})^{-1}_{jj}}\sim\mathcal{N}(0,1)$$
Note that this distributional result only holds asymptotically.

# Generalised linear model

The score test is given by
$$\,$$
$$s(\mathbf{\beta}_2^0)^T(Cov(s({\mathbf{\beta}_2^0)}))^{-1}s(\mathbf{\beta}_2^0)\sim\chi^2_{q}$$

# Generalised linear model

All 3 tests are exact only *asymptotically* and for any finite dataset, they will be only approximate.
$$\,$$

For Gaussian distributions, specifically the general linear model, we can derive exact tests that take into account that we also need to estimate the dispersion parameter (in this case $\sigma^2$).


# General linear model - not for all GLMs

**The F-test**

$H_0:\beta_1=\beta_2=\ldots\beta_p=0$

$H_1:\mbox{ at least one }\beta_j\neq0, j=1,\ldots,p$

$$\,$$

Test statistic: 
$$F=RSS/ESS\sim F_{p,n-p-1}$$

$$\,$$

In general we can test a null hypothesis for $q$ of the $p$ coefficients (ignoring the intercept) to be all zero against one where not all of the $q$ coefficients are zero: $F\sim F_{q,n-p-1}$.


# General linear model - not for all GLMs

**The t-test**

The F test has a special case: when we test only $q=1$ parameter. This is a $F_{1,n-p-1}$ distribution, which turns out to be the square of a $t_{n-p-1}$ distribution.

But you can also derive it directly as a test for the estimated coefficient:


$$H_0:\beta_j=b$$

$$H_1:\beta_j\neq b$$

Test statistic:

$$T=\frac{\hat{\beta_j}-b}{se(\hat{\beta}_j)}\sim T_{n-p-1}$$

# General linear model - not for all GLMs

Also confidence intervals for the general linear model are based on the t-distribution:
$$\,$$

$$\hat{\beta}_j\pm t_{\alpha/2,df=n-p-1}SE(\hat{\beta}_j)$$
$$\,$$

where $SE(\hat{\beta}_j)$ is the square root of the $j^{th}$ diagonal element of $\hat{\sigma}^2(\mathbf{X}^T\mathbf{X})^{-1}$


# Generalised linear model

**Binomial data**

We showed (Practical 2), that the binomial pdf can be written as an exponential family distribution, with the canonical link being the logit function $g(\pi)=\log\left(\frac{\pi}{1-\pi}\right)$.

With binomial data, we usually model the probability parameter $\pi$. So if we measure variables $Y_i\sim\mbox{Bin}(m_i,\pi_i)$, we will use $Y_i/m_i$ as the response variables.

$$g\left(E[\mathbf{Y}/\mathbf{m}|\mathbf{X}]\right)=\log\left(\frac{\mathbf{\pi}}{\mathbf{1}-\mathbf{\pi}}\right)=\mathbf{X}\mathbf{\beta}$$

# Generalised linear model

**Binomial data**

Binomial data can be presented in 2 forms:

* ungrouped; every $Y_i\in\{0,1\}$ and $m_i=1$

* grouped; our data has been grouped (e.g. by class, by household, by village) and we only observe the number of events among $m_i$ trials in each group: $Y_i\in\{0,1,\ldots,m_i\}$ and $m_i\geq1$


# Generalised linear model

**Binomial data**

We can consider other link functions, besides the canonical link function.

Consider, for simplicity, the model with a single predictor:
$$\,$$
$$g(E[Y/m|X])=g(\pi)=\beta_0+\beta_1X$$

# Generalised linear model

**Binomial data**
$$\,$$

* If we use $g(\pi)=\pi$, then a change of 1 in $X$, results in a change of $\beta_1$ in $\pi$. So $\beta_1$ can be interpreted as the **risk difference**.

$$\,$$

* If we use $g(\pi)=\log(\pi)$, then a change of 1 in $X$ results in a change of $\beta_1$ in $\log(\pi)$. Since a difference in logs is the log of the ratio, $\exp(\beta_1)$ can be interpreted as the **risk ratio**.

$$\,$$

* If we use $g(\pi)=\log\left(\frac{\pi}{1-\pi}\right)$, then a unit change in $X$, results in a change of $\beta_1$ in $\log\left(\frac{\pi}{1-\pi}\right)$. A difference in the logs of the odds is the log of the ratio of the odds. So $\exp(\beta_1)$ can be interpreted as the **odds ratio**.


# Generalised linear model

**Binomial data**

Example

```{r, tidy=T}
data(mtcars)
logistReg<-glm(am~wt+hp,family=binomial("logit"),data=mtcars)
coef(logistReg)
```

The odds ratio for a car having manual transmission associated with each increase of 100lbs in weight is $\exp(-8.08/10)$=`r round(digits=2,exp(coef(logistReg)["wt"]/10))` and with each unit increase in horse power is $\exp(0.04)$=`r round(digits=2,exp(coef(logistReg)["hp"]))`.


# Generalised linear model

**Count data**

You can show (**exercise!**) that the Poisson distribution with probability mass function $f_Y(k)=\frac{\lambda^ke^{-\lambda}}{k!}$ belongs to the exponential family with

* canonical parameter $\theta=\log(\lambda)$

* $a(\phi)=1$

* $b(\theta)=\lambda$

* $c(k,\phi)=-\log(k!)$

Further, you can show (also **exercise!**) that the canonical link is
$$g(\lambda)=\log(\lambda)$$

# Generalised linear model

**Count data**

Example
$$\,$$

Prussian army dataset: recorded number of deaths due to horse kick over tie and by corps.
$$\,$$
```{r, echo=F, warning=F, message=F}
library(pscl)
```

```{r, tidy=T, collapse=T}
poisRegNullModel<-glm(y~1,data=prussian,family=poisson)
coef(poisRegNullModel)
```

The average number of deaths by horse kick per year and per corps is $\exp(-0.36)$=`r round(digits=2,exp(coef(poisRegNullModel)["(Intercept)"]))`.

# Generalised linear model

**Count data**

```{r, collapse=T, tidy=T}
poisReg<-glm(y~year+as.factor(corp),data=prussian,family=poisson)
coef(poisReg)
```

```{r, eval=F}
summary(poisReg)
```

No corps significantly worse than the others, no trend with time.


#

[end of ST6103 GLM Session 3]
